{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMfa1XZ4e+3hLeGOitX1jRl"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Diabetic Retinopathy using CNNs**"
      ],
      "metadata": {
        "id": "ekBlOIhwNKYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ce2yUkkd79M6",
        "outputId": "e54574bc-c4ea-4028-f22b-384ddadc9642"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = \"/content/drive/MyDrive/Dataset_DiabeticRetenopathy dataset/Diabetic Retinopathy Dataset CNN\""
      ],
      "metadata": {
        "id": "c16xWRLD8A0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# Step 1: Load the data\n",
        "def load_data(data_dir):\n",
        "    images = []\n",
        "    labels = []\n",
        "    for label in os.listdir(data_dir):\n",
        "        for image_file in os.listdir(os.path.join(data_dir, label)):\n",
        "            image_path = os.path.join(data_dir, label, image_file)\n",
        "            image = cv2.imread(image_path)\n",
        "            image = cv2.resize(image, (128, 128))  # Resize the image if necessary\n",
        "            images.append(image)\n",
        "            labels.append(int(label))\n",
        "    return np.array(images), np.array(labels)\n",
        "\n",
        "# X, y = load_data(r\"C:\\Users\\subba\\Downloads\\Dataset_DiabeticRetenopathy dataset\\Dataset_DiabeticRetenopathy\")\n",
        "X, y = load_data(dataset_path)\n",
        "X = X / 255.0  # Normalize the pixel values"
      ],
      "metadata": {
        "id": "Gfl4TViK-zQj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset is imbalanced so causing Bias in the Model**\n",
        "\n",
        "So the normal train-test splitting shouldn't be done."
      ],
      "metadata": {
        "id": "FosNmVPp_AnN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# Step 2: Split the data into train, validation, and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)  # 0.25 x 0.8 = 0.2\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "9-iB84wi_8LY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To rectify this imbalance, you can use various techniques. One common approach is to balance the class distribution during training. Here's how you can do it:\n",
        "\n",
        "1. **Data Augmentation:** Generate additional synthetic data for the minority class (diseased images) using techniques like rotation, flipping, scaling, etc.\n",
        "2. **Resampling:** Either oversample the minority class (diseased images) or undersample the majority class (non-diseased images) to balance the class distribution.\n",
        "3. **Class Weighting:** Assign higher weights to the minority class during training to make the model pay more attention to it.\n",
        "\n",
        "Let's apply oversampling to balance the class distribution:"
      ],
      "metadata": {
        "id": "XSqz6m1GAQuD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "# Reshape the data for oversampling\n",
        "X_reshaped = X.reshape(X.shape[0], -1)\n",
        "\n",
        "# Apply Random Oversampling\n",
        "oversampler = RandomOverSampler(random_state=42)\n",
        "X_resampled, y_resampled = oversampler.fit_resample(X_reshaped, y)\n",
        "\n",
        "# Reshape the data back to original shape\n",
        "X_resampled = X_resampled.reshape(X_resampled.shape[0], 128, 128, 3)\n",
        "\n",
        "# Split the resampled data into train, validation, and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)  # 0.25 x 0.8 = 0.2\n"
      ],
      "metadata": {
        "id": "XakqXXsH_Wjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dtGvtQPE5MRJ",
        "outputId": "f97e4d48-dc37-4d0c-f1ec-d5b08538be2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.6958 - accuracy: 0.5139\n",
            "Epoch 1: val_accuracy improved from -inf to 0.46897, saving model to best_model.h5\n",
            "14/14 [==============================] - 4s 128ms/step - loss: 0.6958 - accuracy: 0.5139 - val_loss: 0.7066 - val_accuracy: 0.4690\n",
            "Epoch 2/20\n",
            " 5/14 [=========>....................] - ETA: 0s - loss: 0.6846 - accuracy: 0.5875"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 9/14 [==================>...........] - ETA: 0s - loss: 0.6945 - accuracy: 0.5382\n",
            "Epoch 2: val_accuracy did not improve from 0.46897\n",
            "14/14 [==============================] - 0s 19ms/step - loss: 0.6928 - accuracy: 0.5324 - val_loss: 0.6992 - val_accuracy: 0.4690\n",
            "Epoch 3/20\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 0.6870 - accuracy: 0.5170\n",
            "Epoch 3: val_accuracy improved from 0.46897 to 0.61379, saving model to best_model.h5\n",
            "14/14 [==============================] - 0s 20ms/step - loss: 0.6856 - accuracy: 0.5301 - val_loss: 0.6857 - val_accuracy: 0.6138\n",
            "Epoch 4/20\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 0.6709 - accuracy: 0.5938\n",
            "Epoch 4: val_accuracy did not improve from 0.61379\n",
            "14/14 [==============================] - 0s 17ms/step - loss: 0.6697 - accuracy: 0.5972 - val_loss: 0.6983 - val_accuracy: 0.5862\n",
            "Epoch 5/20\n",
            "10/14 [====================>.........] - ETA: 0s - loss: 0.6565 - accuracy: 0.6313\n",
            "Epoch 5: val_accuracy did not improve from 0.61379\n",
            "14/14 [==============================] - 0s 18ms/step - loss: 0.6833 - accuracy: 0.5926 - val_loss: 0.6902 - val_accuracy: 0.5586\n",
            "Epoch 6/20\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 0.6651 - accuracy: 0.6051\n",
            "Epoch 6: val_accuracy improved from 0.61379 to 0.62069, saving model to best_model.h5\n",
            "14/14 [==============================] - 0s 18ms/step - loss: 0.6668 - accuracy: 0.5995 - val_loss: 0.6915 - val_accuracy: 0.6207\n",
            "Epoch 7/20\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 0.6681 - accuracy: 0.6080\n",
            "Epoch 7: val_accuracy improved from 0.62069 to 0.64138, saving model to best_model.h5\n",
            "14/14 [==============================] - 0s 20ms/step - loss: 0.6662 - accuracy: 0.6227 - val_loss: 0.6991 - val_accuracy: 0.6414\n",
            "Epoch 8/20\n",
            "10/14 [====================>.........] - ETA: 0s - loss: 0.6612 - accuracy: 0.6313\n",
            "Epoch 8: val_accuracy did not improve from 0.64138\n",
            "14/14 [==============================] - 0s 18ms/step - loss: 0.6672 - accuracy: 0.6134 - val_loss: 0.6915 - val_accuracy: 0.5310\n",
            "Epoch 9/20\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 0.6548 - accuracy: 0.6165\n",
            "Epoch 9: val_accuracy did not improve from 0.64138\n",
            "14/14 [==============================] - 0s 18ms/step - loss: 0.6543 - accuracy: 0.6157 - val_loss: 0.6863 - val_accuracy: 0.6345\n",
            "Epoch 10/20\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 0.6555 - accuracy: 0.6278\n",
            "Epoch 10: val_accuracy did not improve from 0.64138\n",
            "14/14 [==============================] - 0s 17ms/step - loss: 0.6543 - accuracy: 0.6412 - val_loss: 0.6842 - val_accuracy: 0.5793\n",
            "Epoch 11/20\n",
            "10/14 [====================>.........] - ETA: 0s - loss: 0.6524 - accuracy: 0.6250\n",
            "Epoch 11: val_accuracy did not improve from 0.64138\n",
            "14/14 [==============================] - 0s 17ms/step - loss: 0.6505 - accuracy: 0.6273 - val_loss: 0.6746 - val_accuracy: 0.6000\n",
            "Epoch 12/20\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 0.6569 - accuracy: 0.6335\n",
            "Epoch 12: val_accuracy did not improve from 0.64138\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 0.6476 - accuracy: 0.6435 - val_loss: 0.6655 - val_accuracy: 0.6138\n",
            "Epoch 13/20\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 0.6497 - accuracy: 0.6278\n",
            "Epoch 13: val_accuracy did not improve from 0.64138\n",
            "14/14 [==============================] - 0s 17ms/step - loss: 0.6453 - accuracy: 0.6389 - val_loss: 0.6806 - val_accuracy: 0.6414\n",
            "Epoch 14/20\n",
            "10/14 [====================>.........] - ETA: 0s - loss: 0.6395 - accuracy: 0.6375\n",
            "Epoch 14: val_accuracy did not improve from 0.64138\n",
            "14/14 [==============================] - 0s 18ms/step - loss: 0.6382 - accuracy: 0.6458 - val_loss: 0.6744 - val_accuracy: 0.5862\n",
            "Epoch 15/20\n",
            "10/14 [====================>.........] - ETA: 0s - loss: 0.6232 - accuracy: 0.6656\n",
            "Epoch 15: val_accuracy did not improve from 0.64138\n",
            "14/14 [==============================] - 0s 17ms/step - loss: 0.6293 - accuracy: 0.6574 - val_loss: 0.6594 - val_accuracy: 0.6414\n",
            "Epoch 16/20\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 0.6196 - accuracy: 0.6705\n",
            "Epoch 16: val_accuracy did not improve from 0.64138\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 0.6323 - accuracy: 0.6574 - val_loss: 0.6665 - val_accuracy: 0.6414\n",
            "Epoch 17/20\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 0.6198 - accuracy: 0.6731\n",
            "Epoch 17: val_accuracy did not improve from 0.64138\n",
            "14/14 [==============================] - 0s 21ms/step - loss: 0.6205 - accuracy: 0.6713 - val_loss: 0.6527 - val_accuracy: 0.6345\n",
            "Epoch 18/20\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 0.6114 - accuracy: 0.6705\n",
            "Epoch 18: val_accuracy did not improve from 0.64138\n",
            "14/14 [==============================] - 0s 17ms/step - loss: 0.6060 - accuracy: 0.6736 - val_loss: 0.6957 - val_accuracy: 0.6276\n",
            "Epoch 19/20\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 0.5932 - accuracy: 0.7045\n",
            "Epoch 19: val_accuracy did not improve from 0.64138\n",
            "14/14 [==============================] - 0s 17ms/step - loss: 0.5943 - accuracy: 0.6968 - val_loss: 0.6666 - val_accuracy: 0.6414\n",
            "Epoch 20/20\n",
            "10/14 [====================>.........] - ETA: 0s - loss: 0.5733 - accuracy: 0.7000\n",
            "Epoch 20: val_accuracy improved from 0.64138 to 0.66897, saving model to best_model.h5\n",
            "14/14 [==============================] - 0s 18ms/step - loss: 0.5976 - accuracy: 0.6806 - val_loss: 0.6451 - val_accuracy: 0.6690\n",
            "5/5 [==============================] - 0s 5ms/step\n",
            "Accuracy: 0.6689655172413793\n",
            "Precision: 0.6111111111111112\n",
            "Recall: 0.8088235294117647\n",
            "F1 Score: 0.6962025316455697\n"
          ]
        }
      ],
      "source": [
        "# Step 3: Design the CNN Architecture\n",
        "model = Sequential([\n",
        "    Conv2D(64, (3, 3), activation='relu', padding='same', strides=2, input_shape=(128, 128, 3)),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(128, (3, 3), activation='relu', padding='same', strides=2),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu', padding='same', strides=2),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Flatten(),\n",
        "    Dropout(0.3),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Step 4: Save the best model\n",
        "checkpoint = ModelCheckpoint(\"best_model.h5\", monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_val, y_val), callbacks=callbacks_list)\n",
        "\n",
        "# Step 5: Load the best model\n",
        "best_model = load_model(\"best_model.h5\")\n",
        "\n",
        "# Step 6: Predict the test data\n",
        "y_pred = best_model.predict(X_test)\n",
        "y_pred = np.round(y_pred).flatten()\n",
        "\n",
        "# Step 7: Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n"
      ]
    }
  ]
}